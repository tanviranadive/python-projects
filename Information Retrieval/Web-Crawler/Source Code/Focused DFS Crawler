from bs4 import BeautifulSoup

from collections import deque
import requests
import re
import sys
import codecs
import urllib2
reload(sys)
import time
from urlparse import urljoin
sys.setdefaultencoding('utf-8')

f = open('DFSUrls.txt','w').close()


seedurl = "https://en.wikipedia.org/wiki/Sustainable_energy"
keyword = "solar"
frontier=[]
frontier.append(seedurl)
visited_List = []
matched_List = []
urlList = []
#depth = 1
count = 0
f = open('DFSUrls.txt', 'a')
regex = re.compile(r'^/wiki/', re.IGNORECASE)
# Create stack
stack = []
stack.append(seedurl)


# crawl function

def crawl(url, d):
	if d == 5:
		print "at depth 5 and returning"
		return
	
	elif len(matched_List) < 1000:
		print "At depth--------" + str(d)
		print "Returning"
		time.sleep(1)
		r1  = requests.get(url)
		if r1.status_code == 200:
			"Got requests"
			data1 = r1.text
			soup1 = BeautifulSoup(data1)
			for link1 in soup1.find_all("a"):
				"Link working"
				if (regex.match(str(link1.get("href")))):
					url1 = link1.get('href')
					if (":" not in url1) and ("#" not in url1) and (link1.get('class')!="img") and (url1 not in visited_List) :
						completeUrl1 = urljoin("http://en.wikipedia.org", url1)
						visited_List.append(completeUrl1)
						if ((re.search(keyword, url1, re.IGNORECASE)) or (re.search(keyword, link1.text, re.IGNORECASE))) and (completeUrl1 not in matched_List):
							
							print completeUrl1
							stack.append(completeUrl1)

							
							if len(matched_List) < 1000 and (completeUrl1 not in matched_List):
								matched_List.append(completeUrl1)
							
								print "going for depth -----" + str(d+1)
								time.sleep(1)
								crawl(completeUrl1, d+1)
							else:
								break	
		else:
			return	

#Main part	

def main():
	time.sleep(1)
	r  = requests.get(seedurl)
	if r.status_code == 200:
		data = r.text
		soup = BeautifulSoup(data)
		for link in soup.find_all("a"):
			
			if (regex.match(str(link.get("href")))):
						url = link.get('href')
						
						if (":" not in url) and ("#" not in url) and (link.get('class')!="img") and (url not in visited_List):
							completeUrl = urljoin("http://en.wikipedia.org", url)
							
							if (re.search(keyword, url, re.IGNORECASE)) or (re.search(keyword, link.text, re.IGNORECASE)):
								urlList.append(completeUrl)
															

		print len(urlList)
		#print count
		#for i in urlList:
		#	print i

		for url in urlList:
			visited_List.append(url)
			if len(matched_List) >= 1000:
				break
			elif (url not in matched_List):
				matched_List.append(url)
				print "calling URL,2-------"
				time.sleep(1)
				crawl(url, 2)
			else:
				continue	

		#for v in visited_List:
	#		print v	
	else:
		return
		

		for m in matched_List:
			print m

		print "Visited list count------"
		print len(visited_List)	

		print "Matched list count-------" 
		print len(matched_List)	


if __name__== '__main__':
    main()
					
with open("DFSUrls.txt", "a") as DFSFile: 
		print DFSFile   

   		for list in matched_List:
   			print list
   			DFSFile.write("\n" + str(list))

DFSFile.close()					

	 
