from bs4 import BeautifulSoup

from collections import deque
import requests
import re
import sys
import codecs
import urllib2
reload(sys)
from urlparse import urljoin
sys.setdefaultencoding('utf-8')
import time

seedurl = "https://en.wikipedia.org/wiki/Sustainable_energy"
website = seedurl
keyword = "solar"
frontier=[]
frontier.append(seedurl)
visited_List = []
matched_List = []
countdepth = [0] * 5
#depth = 1
depth = 2
countdepth[0] = 1
regex = re.compile(r'^/wiki/', re.IGNORECASE)
# Create queue
queue = []
queue.append(seedurl)
visited_List.append(seedurl)
matched_List.append(seedurl)
frontier.append(seedurl)

f = open('BFSUrls.txt', 'w')


def main():
	
	crawl(seedurl, keyword, depth)

	print len(matched_List)
	print len(frontier)
	print "Matched list urls----"
	for m in matched_List:
		print m

	with open("BFSList.txt", "a") as f: 
		print f
		for list in matched_List:
			f.write("\n" + str(list))

	f.close()	



def crawl(url, keyword, depth):

	if url == seedurl:
			depth = 2
			queue.pop(0)

	elif depth > 5 or len(matched_List) >= 1000:
		"Depth is more than 5 or matched List max reached"
		return
	
	print "inside Crawl function url ------" + url 
	print "At depth ----" + str(depth)
	time.sleep(1)

	r  = requests.get(url)
	if r.status_code == 200:
		data = r.text
		soup = BeautifulSoup(data)

		for link in soup.find_all("a"):
			if (regex.match(str(link.get("href")))):
				url = link.get('href')
				if (":" not in url) and ("#" not in url):	
					completeUrl = urljoin("http://en.wikipedia.org", url)
					
					if len(matched_List) < 1000:

						if ((re.search(keyword, url, re.IGNORECASE)) or (re.search(keyword, link.text, re.IGNORECASE))) and (completeUrl not in matched_List) and (completeUrl not in visited_List):
							
							matched_List.append(completeUrl)
							queue.append(completeUrl)
							frontier.append(completeUrl)
							print " Matched url and adding to queue ----" + completeUrl
							print "Fetching urls at depth and adding to queue ----" + str(depth)
							print depth
							
							countdepth[depth-1] = countdepth[depth - 1] + 1

					else:
						break

		print "Number of urls at depth " +  str(depth) +"---> " + str(countdepth[depth-1])
		while countdepth[depth-1] != 0:
			
			if len(matched_List) < 1000 and depth <=5 :

				nextUrl = queue.pop(0)
				print "Now crawling URL ----" + nextUrl
				internalCrawler(nextUrl,keyword,depth+1)
				visited_List.append(nextUrl)
				countdepth[depth-1] = countdepth[depth-1] - 1
			else:
				break

		if countdepth[depth-1] == 0:
			depth = depth + 1
			print "Now depth increases ---" + str(depth)
			url = queue.pop()
			print "New url to crawl --- " + url
			visited_List.append(url)
			crawl(url, keyword, depth)
	else:
		return		



def internalCrawler(nextUrl, keyword, d):
	print "Inside internal URL---" + nextUrl
	time.sleep(1)
	r1  = requests.get(nextUrl)
	if r1.status_code == 200:
		dataInternal = r1.text
		soup1 = BeautifulSoup(dataInternal)
		for link1 in soup1.find_all("a"):
			#count the number of urls visited
			"Fetching links in internal crawler"	
			url1 = link1.get('href')
					
			if (regex.match(str(url1))):
				
				if (":" not in url1) and ("#" not in url1):
						
					completeUrl1 = urljoin("http://en.wikipedia.org", url1)
					if len(matched_List) < 1000:
						if ((re.search(keyword, url1, re.IGNORECASE)) or (re.search(keyword, link1.text, re.IGNORECASE))) and (completeUrl1 not in matched_List) and (completeUrl1 not in visited_List):


							print "Inside internal crawler --- matched url --" + completeUrl1	
							matched_List.append(completeUrl1)
							queue.append(completeUrl1)
							frontier.append(completeUrl1)			
							countdepth[d-1] = countdepth[d-1] + 1

					else:
						break
	else:
		return					

if __name__== '__main__':
    main()						